{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa334b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.8\n",
      "  Downloading tensorflow_gpu-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (1.20.1)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (1.15.0)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (52.0.0.post20210125)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (0.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (1.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (0.2.0)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (3.19.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (1.41.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (1.12)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorflow-gpu==2.8) (3.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-gpu==2.8) (0.36.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (2.3.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (3.3.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\araqj\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8) (3.1.1)\n",
      "Installing collected packages: tf-estimator-nightly, tensorflow-io-gcs-filesystem, tensorboard, libclang, keras, tensorflow-gpu\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.6.0\n",
      "    Uninstalling keras-2.6.0:\n",
      "      Successfully uninstalled keras-2.6.0\n",
      "Successfully installed keras-2.8.0 libclang-13.0.0 tensorboard-2.8.0 tensorflow-gpu-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow-gpu==2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0e113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!pip install plotly\n",
    "!pip install --upgrade nbformat\n",
    "!pip install nltk\n",
    "!pip install spacy # spaCy is an open-source software library for advanced natural language processing\n",
    "!pip install WordCloud\n",
    "!pip install jupyterthemes\n",
    "!pip install gensim # Gensim is an open-source library for unsupervised topic modeling and natural language processing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "# import keras\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False) \n",
    "# setting the style of the notebook to be monokai theme  \n",
    "# this line of code is important to ensure that we are able to see the x and y axes clearly\n",
    "# If you don't run this code line, you will notice that the xlabel and ylabel on any plot is black on black and it will be hard to see them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e94751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_true = pd.read_csv(\"True.csv\")\n",
    "df_fake = pd.read_csv(\"Fake.csv\")\n",
    "\n",
    "#datasets available at https://drive.google.com/drive/folders/1MSBCU6BA9HXvQHKm5qARNJl0XID-mNej?usp=sharing\n",
    "\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28122260",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c43cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd41cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c74f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1127f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48740178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Exploratory Analysis\n",
    "\n",
    "# add a target class column to indicate whether the news is real or fake\n",
    "df_true['isfake'] = 0\n",
    "df_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake['isfake'] = 1\n",
    "df_fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Real and Fake News\n",
    "df = pd.concat([df_true, df_fake]).reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2af6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['date'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24deb79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine title and text together\n",
    "df['original'] = df['title'] + ' ' + df['text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e922bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['original'][11000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e67cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Data Cleaning\n",
    "# download stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc377b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain additional stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words2 = stopwords.words('spanish')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and remove words with 2 or less characters\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
    "            result.append(token)            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the dataframe\n",
    "df['clean'] = df['original'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4fe2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original news\n",
    "df['original'][11000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2076e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show cleaned up news after removing stopwords\n",
    "print(df['clean'][11000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61429538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the total words present in the dataset\n",
    "list_of_words = []\n",
    "for i in df.clean:\n",
    "    for j in i:\n",
    "        list_of_words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a42eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_words\n",
    "len(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the total number of unique words\n",
    "total_words = len(list(set(list_of_words))) #set, only unique ones\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab40ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the words into a string\n",
    "df['clean_joined'] = df['clean'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aaeec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f003364",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_joined'][11000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Visualize Cleaned Up Dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebe325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of samples in 'subject'\n",
    "plt.figure(figsize = (8, 8))\n",
    "sns.countplot(y = \"subject\", data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db510e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "sns.countplot(y = \"isfake\", data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913db2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the word cloud for text that is Real\n",
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.isfake == 1].clean_joined))\n",
    "plt.imshow(wc, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72107e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the word cloud for text that is Fake\n",
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.isfake == 0].clean_joined))\n",
    "plt.imshow(wc, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5eb535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of maximum document will be needed to create word embeddings \n",
    "maxlen = -1\n",
    "for doc in df.clean_joined:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in any document is =\", maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a884411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distribution of number of words in a text\n",
    "import plotly.express as px\n",
    "fig = px.histogram(x = [len(nltk.word_tokenize(x)) for x in df.clean_joined], nbins = 100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dabecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test and train \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.clean_joined, df.isfake, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18eaea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer to tokenize the words and create sequences of tokenized words\n",
    "tokenizer = Tokenizer(num_words = total_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46338a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The encoding for document\\n\",df.clean_joined[11000],\"\\n is : \",train_sequences[11000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1445c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding can either be maxlen = 4406 or smaller number maxlen = 40 seems to work well based on results\n",
    "padded_train = pad_sequences(train_sequences,maxlen = 40, padding = 'post', truncating = 'post')\n",
    "padded_test = pad_sequences(test_sequences,maxlen = 40, truncating = 'post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bdcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,doc in enumerate(padded_train[:3]):\n",
    "     print(\"The padded encoding for document\",i+1,\" is : \",doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a3859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build and Train the Model\n",
    "# Sequential Model\n",
    "model = Sequential()\n",
    "\n",
    "# embeddidng layer\n",
    "model.add(Embedding(total_words, output_dim = 128))\n",
    "# model.add(Embedding(total_words, output_dim = 240))\n",
    "\n",
    "\n",
    "# Bi-Directional RNN and LSTM\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(1,activation= 'sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fea89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8b00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit(padded_train, y_train, batch_size = 64, validation_split = 0.1, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f831d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Assess Trained Model Performance\n",
    "# make prediction\n",
    "pred = model.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the predicted value is >0.5 it is real else it is fake\n",
    "prediction = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i].item() > 0.5:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(list(y_test), prediction)\n",
    "\n",
    "print(\"Model Accuracy : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(list(y_test), prediction)\n",
    "plt.figure(figsize = (15, 15))\n",
    "sns.heatmap(cm, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03471b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category dict\n",
    "category = { 0: 'Fake News', 1 : \"Real News\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model was based on Coursera's Fake News Detection With Machine Learning guided project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
